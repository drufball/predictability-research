# predictability-research
This research explores various ways to slice the data generated by the ecosystem infra team's API confluence tool. API confluence crawls the Javascript object graph in as many versions of Chrome, Safari, Firefox, and Edge that we can access. It records the presence or absence of each interface/API.

Currently, the only analysis we run is `first-to-ship.js`.

# First to ship results
For this analysis, we look at every API and determine the order of implementations (`sanitized-api-data.js` does this). From this we can compute, for each browser, how many APIs it was the first to ship. In other words, how much interop risk is that browser taking betting on new features?

From there, we look at how many of those first implementations ultimately led to a second implementation. In other words, how many of those bets on new features paid off?

We then calculate the average time it took for a second implementation to appear.

__Chrome__:
- Shipped 1161 to 1353 APIs first
- 45-53% of APIs shipped first have another implementation
- Average time to second implementation: 132 days
- Median time to second implementation: 123 days

__Firefox__:
- Shipped 1444 to 1520 APIs first
- 30-34% of APIs shipped first have another implementation
- Average time to second implementation: 172 days
- Median time to second implementation: 1 days

__Edge__:
- Shipped 1261 to 1261 APIs first
- 20-20% of APIs shipped first have another implementation
- Average time to second implementation: 391 days
- Median time to second implementation: 421 days

__Safari__:
- Shipped 824 to 900 APIs first
- 6-14% of APIs shipped first have another implementation
- Average time to second implementation: 472 days
- Median time to second implementation: 148 days

# First to ship results normalized
For this analysis, we do the same as the first to ship results, except each time a browser ships an API first, we give it a score of 1 / {# of APIs in the interface it belongs to}.
The reasoning behind this is account interfaces that have a large amount of APIs and to make sure our analysis isn't skewed by a small set of interfaces that dominate all the API counts. We call each of these units an "interface adjusted count" :)

__Chrome__:
- Shipped 125.30 to 139.72 interface adjusted counts first
- 43-49% of interface adjusted counts shipped first have another implementation

__Firefox__:
- Shipped 49.17 to 58.42 interface adjusted counts first
- 31-42% of interface adjusted counts shipped first have another implementation

__Edge__:
- Shipped 123.37 to 123.37 interface adjusted counts first
- 20-20% of interface adjusted counts shipped first have another implementation

__Safari__:
- Shipped 57.40 to 64.65 interface adjusted counts first
- 8-18% of interface adjusted counts shipped first have another implementation

# Caveats
We do not count any APIs that were implemented in the first version available in our dataset for Chrome, Safari, or Firefox. This is because we can't tell who was the first to implement.

Additionally, there are several cases where an API first appears in one browser, but the earliest data we have for a different browser is after that appearance. In this case, we don't know whether the API just appeared in the second browser, or if it's been there for a long time. This is why we provide a range of possible "first to ship" values.
